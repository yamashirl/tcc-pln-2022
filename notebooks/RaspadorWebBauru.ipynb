{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "import re\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "DIRETORIO_PDF = 'BauruPDF/'\n",
    "DIRETORIO_TXT = 'BauruTXT/'\n",
    "DIRETORIO_PAR = 'BauruPAR/'\n",
    "DIRETORIO_DOC = 'BauruDOC/'\n",
    "DIRETORIO_JSON = 'BauruJSON/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_links_diarios_oficiais(ano, mes):\n",
    "    assert type(ano) == str\n",
    "    assert type(mes) == str\n",
    "    \n",
    "    lista_links = []\n",
    "    \n",
    "    resposta = requests.get(f'https://www2.bauru.sp.gov.br/juridico/diariooficial.aspx?a={ano}&m={mes}')\n",
    "    if resposta.status_code != 200:\n",
    "        print('Página retornou erro', resposta.status_code)\n",
    "        return\n",
    "    \n",
    "    soup = BeautifulSoup(resposta.text, 'html.parser')\n",
    "    \n",
    "    itens = soup.find_all('li')\n",
    "    for item in itens:\n",
    "        ancora = item.find('a')\n",
    "        if ancora != None:\n",
    "            link = ancora['href']\n",
    "            if path.splitext(link)[1].lower() == '.pdf':\n",
    "                lista_links.append(link)\n",
    "    \n",
    "    return lista_links\n",
    "\n",
    "def obter_diario_oficial(link, diretorio_destino = DIRETORIO_PDF):\n",
    "    assert type(link) == str\n",
    "    nome_arquivo = path.basename(link)\n",
    "    assert not path.isfile(path.join(diretorio_destino, nome_arquivo))\n",
    "    \n",
    "    resposta = requests.get(f'https://www2.bauru.sp.gov.br{link}')\n",
    "    if resposta.status_code != 200:\n",
    "        print('Página retornou erro', resposta.status_code)\n",
    "        return\n",
    "    \n",
    "    with open(path.join(diretorio_destino, nome_arquivo), 'wb') as arquivo:\n",
    "        arquivo.write(resposta.content)\n",
    "        \n",
    "    return nome_arquivo\n",
    "\n",
    "def obter_lista_de_dos(links, diretorio_destino = DIRETORIO_PDF, tempo_espera = 0.1):\n",
    "    arquivos = []\n",
    "    for link in tqdm(links):\n",
    "        nome = obter_diario_oficial(link, diretorio_destino = diretorio_destino)\n",
    "        arquivos.append(nome)\n",
    "        time.sleep(tempo_espera)\n",
    "    \n",
    "    return arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_tabela_licitacoes(t = 1):\n",
    "    \"\"\"\n",
    "    Obtém os dados resumidos da página de licitações abertas.\n",
    "\n",
    "    :keyword t int: id interno da tabela.\n",
    "        * 'Licitações Abertas' se t = 1;\n",
    "        * 'Licitações Suspensas' se t = 2;\n",
    "        * 'Licitações Encerradas' se t = 3.\n",
    "\n",
    "    :returns: DataFrame com os dados da tabela\n",
    "    \"\"\"\n",
    "    \n",
    "    # Obter a página de:\n",
    "\n",
    "    resposta = requests.get(f'https://www2.bauru.sp.gov.br/administracao/licitacoes/licitacoes.aspx?t={t}')\n",
    "    if resposta.status_code != 200:\n",
    "        print('Página retornou erro', resposta.status_code)\n",
    "        return\n",
    "    \n",
    "    soup = BeautifulSoup(resposta.text, 'html.parser')\n",
    "    \n",
    "    # Buscar a tabela das licitações e obter o corpo\n",
    "    licitacao_table = soup.find('table')\n",
    "    # Não há dados relevantes no cabeçalho da tabela.\n",
    "    #cabecalho_tabela = licitacao_table.thead\n",
    "    conteudo_tabela = licitacao_table.tbody\n",
    "    \n",
    "    # Para cada linha (child em conteudo_tabela, ou <tr />) da tabela,\n",
    "    #   extrair as três colunas (contents[0..2], <td />)\n",
    "    licitacoes = []\n",
    "    extrator_numero_ano = re.compile('(?P<numero>\\d+)/(?P<ano>\\d{2,4})')\n",
    "    extrator_link       = re.compile('licitacoes_detalhes\\.aspx\\?l\\=(?P<numero_link>\\d+)')\n",
    "    \n",
    "    for child in conteudo_tabela:\n",
    "        objeto       = child.contents[0].a.text\n",
    "        modalidade   = child.contents[1].a.contents[0]\n",
    "\n",
    "        numero_re = extrator_numero_ano.search(child.contents[1].a.contents[2])\n",
    "        modalidade_numero = numero_re.group('numero')\n",
    "        ano = numero_re.group('ano')\n",
    "\n",
    "        interessados = child.contents[2].a.text\n",
    "\n",
    "        link_re = extrator_link.search(child.contents[0].a['href'])\n",
    "        numero_link = link_re.group('numero_link')\n",
    "\n",
    "        licitacoes.append({'modalidade': modalidade,\n",
    "                           'modalidade_numero': modalidade_numero,\n",
    "                           'ano': ano,\n",
    "                           'interessado': interessados,\n",
    "                           'objeto': objeto,\n",
    "                           'link': numero_link})\n",
    "\n",
    "    df_licitacoes = pd.DataFrame(licitacoes)\n",
    "    return df_licitacoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "extrator_numero_ano = re.compile('((?P<numero>\\d{1,3}(\\.\\d{3})?)/(?P<ano>\\d{2,4}))')\n",
    "extrator_nome_numero_ano = re.compile('((?P<nome>\\D+)\\s+(?P<numero>\\d{1,3}(\\.\\d{3})?)/(?P<ano>\\d{2,4}))')\n",
    "extrator_anexo = re.compile('Anexo (?P<numero>\\d+) - (?P<descricao>.*)')\n",
    "extrator_publicacao = re.compile('(?P<dia>\\d{1,2})/(?P<mes>\\d{1,2})/(?P<ano>\\d{2,4})\\s*:\\s*(?P<titulo>(\\s?\\w)+)\\s*:')\n",
    "extrator_data = re.compile('(?P<hora>\\d{1,2}):(?P<minuto>\\d{1,2}) horas do dia (?P<dia>\\d{1,2}) de (?P<mes>\\w+) de (?P<ano>\\d{2,4}) \\((\\w+-feira|sábado|domingo)\\)')\n",
    "\n",
    "def extrair_data(s):\n",
    "    resultado = extrator_data.search(s)\n",
    "    if type(resultado) != None:\n",
    "        return {\n",
    "            'ano': resultado.group('ano'),\n",
    "            'mes': resultado.group('mes'),\n",
    "            'dia': resultado.group('dia'),\n",
    "            'hora': resultado.group('hora'),\n",
    "            'minuto': resultado.group('minuto')\n",
    "        }\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "def obter_detalhes_licitacao(identificador):\n",
    "    \"\"\"\n",
    "    Obtém os dados detalhados de uma licitação a partir do código interno do site\n",
    "    \n",
    "    :param identificador int: código indentificador interno do site\n",
    "    :returns: dict com os dados extraídos\n",
    "    \"\"\"\n",
    "    \n",
    "    # Adquirir a página de uma licitação\n",
    "    response = requests.get(f'https://www2.bauru.sp.gov.br/administracao/licitacoes/licitacoes_detalhes.aspx?l={identificador}')\n",
    "    if response.status_code != 200:\n",
    "        print('Página retornou erro', response.status_code)\n",
    "        return\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    conteudo = soup.find('main').find('div', class_ = 'col-10')\n",
    "\n",
    "    detalhes_dict = {\n",
    "        'identificador': identificador\n",
    "    }\n",
    "\n",
    "    for linha in conteudo.find_all('div', class_ = 'row'):\n",
    "        if len(linha.contents) == 1:\n",
    "            resultado = extrator_nome_numero_ano.search(linha.text)\n",
    "            detalhes_dict['titulo'] = {\n",
    "                'modalidade': resultado.group('nome'),\n",
    "                'numero': resultado.group('numero'),\n",
    "                'ano':    resultado.group('ano')\n",
    "            }\n",
    "\n",
    "        elif len(linha.contents) == 2:\n",
    "            indice = linha.find('div', class_ = 'col-md-2').text\n",
    "            valores = linha.find('div', class_ = 'col-md-10')\n",
    "\n",
    "            if indice == 'Tipo:':\n",
    "                indice = 'tipo'\n",
    "                detalhes_dict[indice] = valores.text\n",
    "                \n",
    "            elif indice == 'Interessado:':\n",
    "                indice = 'interessado'\n",
    "                detalhes_dict[indice] = valores.text\n",
    "                \n",
    "            elif indice == 'Processo:':\n",
    "                indice = 'processo'\n",
    "                resultado = extrator_numero_ano.search(valores.text)\n",
    "                detalhes_dict[indice] = {\n",
    "                    'numero': resultado.group('numero'),\n",
    "                    'ano': resultado.group('ano')\n",
    "                }\n",
    "                \n",
    "            elif indice == 'Especificação:':\n",
    "                indice = 'especificacao'\n",
    "                detalhes_dict[indice] = valores.text\n",
    "                \n",
    "            elif indice == 'Prazo para Recebimento Propostas:':\n",
    "                indice = 'prazo_recebimento_propostas'\n",
    "                detalhes_dict[indice] = extrair_data(valores.text)\n",
    "                \n",
    "            elif indice == 'Prazo para Apresentação de Propostas:':\n",
    "                indice = 'prazo_apresentacao_propostas'\n",
    "                detalhes_dict[indice] = extrair_data(valores.text)\n",
    "                \n",
    "            elif indice == 'Prazo para Entrega dos Envelopes:':\n",
    "                indice = 'prazo_entrega_envelopes'\n",
    "                detalhes_dict[indice] = extrair_data(valores.text)\n",
    "\n",
    "            elif indice == 'Processo Tribunal de Contas:':\n",
    "                indice = 'processo_tribunal_de_contas'\n",
    "                detalhes_dict[indice] = valores.text\n",
    "                \n",
    "            elif indice == 'Data de vencimento:':\n",
    "                indice = 'data_vencimento'\n",
    "                detalhes_dict[indice] = extrair_data(valores.text)\n",
    "                \n",
    "            elif indice == 'Data:':\n",
    "                indice = 'data'\n",
    "                detalhes_dict[indice] = extrair_data(valores.text)\n",
    "            \n",
    "            elif indice == 'Observação:':\n",
    "                indice = 'observacao'\n",
    "                detalhes_dict[indice] = valores.text\n",
    "                \n",
    "            elif indice == 'Processo Apensado' or indice == 'Processos Apensados':\n",
    "                indice = 'processos_apensados'\n",
    "                \n",
    "                if 'processos_apensados' not in detalhes_dict:\n",
    "                    detalhes_dict['processos_apensados'] = []\n",
    "                \n",
    "                matches = extrator_numero_ano.finditer(valores.text)\n",
    "                for p in matches:\n",
    "                    detalhes_dict['processos_apensados'].append({\n",
    "                        'numero': p.group('numero'),\n",
    "                        'ano': p.group('ano')\n",
    "                    })\n",
    "                \n",
    "            elif indice == 'Documentos:':\n",
    "                indice = 'documentos'\n",
    "\n",
    "                if 'documentos' not in detalhes_dict:\n",
    "                    detalhes_dict['documentos'] = []\n",
    "\n",
    "                for d in valores.find_all('li'):\n",
    "                    if d.b.string[0] == 'E':\n",
    "                        # Assume 'Edital xxx/xxxx'\n",
    "                        resultado = extrator_nome_numero_ano.search(d.b.string)\n",
    "                        detalhes_dict['documentos'].append({\n",
    "                            'nome': resultado.group('nome'),\n",
    "                            'numero': resultado.group('numero'),\n",
    "                            'ano': resultado.group('ano'),\n",
    "                            'link': d.a['href']\n",
    "                        })\n",
    "                    elif d.b.string[0] == 'A':\n",
    "                        # Assume 'Anexo x - [...]'\n",
    "                        resultado = extrator_anexo.search(d.b.string)\n",
    "                        detalhes_dict['documentos'].append({\n",
    "                            'nome': 'Anexo',\n",
    "                            'numero': resultado.group('numero'),\n",
    "                            'descricao': resultado.group('descricao'),\n",
    "                            'link': d.a['href']\n",
    "                        })\n",
    "                    else:\n",
    "                        # Situação inesperada??\n",
    "                        print('Situação Inesperada analisando documentos. \"Anexo\" ou \"Edital\" não encontrados.', detalhes_dict['titulo'])\n",
    "                        detalhes_dict['documentos'].append((d.b.string, d.a['href']))\n",
    "\n",
    "            elif indice == 'Publicações:':\n",
    "                indice = 'publicacoes'\n",
    "\n",
    "                if 'publicacoes' not in detalhes_dict:\n",
    "                    detalhes_dict['publicacoes'] = []\n",
    "\n",
    "                for i in range(0, len(valores.contents), 3):\n",
    "                    resultado = extrator_publicacao.search(valores.contents[i].string)\n",
    "                    publicacao = {\n",
    "                        'titulo': resultado.group('titulo'),\n",
    "                        'dia': resultado.group('dia'),\n",
    "                        'mes': resultado.group('mes'),\n",
    "                        'ano': resultado.group('ano'),\n",
    "                        'conteudo': valores.contents[i + 1]\n",
    "                    }\n",
    "                    detalhes_dict['publicacoes'].append(publicacao)\n",
    "                    \n",
    "            else:\n",
    "                if 'misc' not in detalhes_dict:\n",
    "                    detalhes_dict['misc'] = []\n",
    "                \n",
    "                detalhes_dict['misc'].append((indice, str(valores)))\n",
    "                \n",
    "            \n",
    "        else:\n",
    "            print('quantidade de valores inesperado:')\n",
    "            print(linha)\n",
    "    \n",
    "    return detalhes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "licitacoes_df = obter_tabela_licitacoes(t = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baixar_licitacoes(df, tempo_espera = 0.1, diretorio = DIRETORIO_JSON):\n",
    "    \"\"\"\n",
    "    Baixa todos as licitações num dataframe\n",
    "    \n",
    "    :param df DataFrame: DataFrame com todas as licitações a baixar. Será utilizado a Series 'link'.\n",
    "    :keyword tempo_espera int: Tempo de espera entre o download de uma licitação e outra. Utilizado para não enviar muitas requisições em um curto período de tempo.\n",
    "    :keyword diretorio str: Diretorio de destino para os arquivos .json baixados.\n",
    "    \"\"\"\n",
    "    \n",
    "    for link_licitacao in tqdm(df['link'].values):\n",
    "        #assert path.isfile(path.join(diretorio, link_licitacao + '.json')) == False,      f'Arquivo {link_licitacao}.json já existe'\n",
    "        if path.isfile(path.join(diretorio, link_licitacao + '.json')):\n",
    "            continue\n",
    "        \n",
    "        dict_licitacao = obter_detalhes_licitacao(link_licitacao)\n",
    "        \n",
    "        with open(path.join(diretorio, link_licitacao + '.json'), 'w') as arquivo:\n",
    "            json.dump(dict_licitacao, arquivo, ensure_ascii = False)\n",
    "\n",
    "        time.sleep(tempo_espera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:00<00:00, 31770.53it/s]\n",
      "100%|██████████| 452/452 [00:00<00:00, 53886.23it/s]\n",
      "100%|██████████| 379/379 [00:00<00:00, 46901.76it/s]\n",
      "100%|██████████| 363/363 [00:00<00:00, 36817.05it/s]\n",
      "100%|██████████| 293/293 [00:00<00:00, 34050.90it/s]\n",
      "100%|██████████| 300/300 [00:31<00:00,  9.56it/s] \n",
      "100%|██████████| 311/311 [02:32<00:00,  2.04it/s]\n"
     ]
    }
   ],
   "source": [
    "baixar_licitacoes(licitacoes_df[licitacoes_df['ano'] == '2022'])\n",
    "baixar_licitacoes(licitacoes_df[licitacoes_df['ano'] == '2021'])\n",
    "baixar_licitacoes(licitacoes_df[licitacoes_df['ano'] == '2020'])\n",
    "baixar_licitacoes(licitacoes_df[licitacoes_df['ano'] == '2019'])\n",
    "baixar_licitacoes(licitacoes_df[licitacoes_df['ano'] == '2018'])\n",
    "baixar_licitacoes(licitacoes_df[licitacoes_df['ano'] == '2017'])\n",
    "baixar_licitacoes(licitacoes_df[licitacoes_df['ano'] == '2016'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147/147 [05:45<00:00,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['do_20161231_2773.pdf', 'do_20161229_2772.pdf', 'do_20161227_2771.pdf', 'do_20161224_2770.pdf', 'do_20161222_2769.pdf', 'do_20161220_2768.pdf', 'do_20161217_2767.pdf', 'do_20161215_2766.pdf', 'do_20161213_2765.pdf', 'do_20161210_2764.pdf', 'do_20161208_2763.pdf', 'do_20161206_2762.pdf', 'do_20161203_2761.pdf', 'do_20161201_2760.pdf', 'do_20161129_2759.pdf', 'do_20161126_2758.pdf', 'do_20161124_2757.pdf', 'do_20161122_2756.pdf', 'do_20161119_2755.pdf', 'do_20161117_2754.pdf', 'do_20161112_2753.pdf', 'do_20161110_2752.pdf', 'do_20161108_2751.pdf', 'do_20161105_2750.pdf', 'do_20161101_2749.pdf', 'do_20161027_2748.pdf', 'do_20161025_2747.pdf', 'do_20161022_2746.pdf', 'do_20161020_2745.pdf', 'do_20161018_2744.pdf', 'do_20161015_2743.pdf', 'do_20161013_2742.pdf', 'do_20161011_2741.pdf', 'do_20161008_2740.pdf', 'do_20161006_2739.pdf', 'do_20161004_2738.pdf', 'do_20161001_2737.pdf', 'do_20160929_2736.pdf', 'do_20160927_2735.pdf', 'do_20160924_2734.pdf', 'do_20160922_2733.pdf', 'do_20160920_2732.pdf', 'do_20160917_2731.pdf', 'do_20160915_2730.pdf', 'do_20160913_2729.pdf', 'do_20160910_2728.pdf', 'do_20160906_2727.pdf', 'do_20160903_2726.pdf', 'do_20160901_2725.pdf', 'do_20160830_2724.pdf', 'do_20160827_2723.pdf', 'do_20160825_2722.pdf', 'do_20160823_2721.pdf', 'do_20160820_2720.pdf', 'do_20160818_2719.pdf', 'do_20160816_2718.pdf', 'do_20160813_2717.pdf', 'do_20160811_2716.pdf', 'do_20160809_2715.pdf', 'do_20160806_2714.pdf', 'do_20160804_2713.pdf', 'do_20160730_2712.pdf', 'do_20160728_2711.pdf', 'do_20160726_2710.pdf', 'do_20160723_2709.pdf', 'do_20160721_2708.pdf', 'do_20160719_2707.pdf', 'do_20160716_2706.pdf', 'do_20160714_2705.pdf', 'do_20160712_2704.pdf', 'do_20160709_2703.pdf', 'do_20160707_2702.pdf', 'do_20160705_2701.pdf', 'do_20160702_2700.pdf', 'do_20160630_2699.pdf', 'do_20160628_2698.pdf', 'do_20160625_2697.pdf', 'do_20160623_2696.pdf', 'do_20160621_2695.pdf', 'do_20160618_2694.pdf', 'do_20160616_2693.pdf', 'do_20160614_2692.pdf', 'do_20160611_2691.pdf', 'do_20160609_2690.pdf', 'do_20160607_2689.pdf', 'do_20160604_2688.pdf', 'do_20160602_2687.pdf', 'do_20160531_2686.pdf', 'do_20160526_2685.pdf', 'do_20160524_2684.pdf', 'do_20160521_2683.pdf', 'do_20160519_2682.pdf', 'do_20160517_2681.pdf', 'do_20160514_2680.pdf', 'do_20160512_2679.pdf', 'do_20160510_2678.pdf', 'do_20160507_2677.pdf', 'do_20160505_2676.pdf', 'do_20160503_2675.pdf', 'do_20160430_2674.pdf', 'do_20160428_2673.pdf', 'do_20160426_2672.pdf', 'do_20160421_2671.pdf', 'do_20160419_2670.pdf', 'do_20160416_2669.pdf', 'do_20160414_2668.pdf', 'do_20160412_2667.pdf', 'do_20160409_2666.pdf', 'do_20160407_2665.pdf', 'do_20160405_2664.pdf', 'do_20160402_2663.pdf', 'do_20160331_2662.pdf', 'do_20160329_2661.pdf', 'do_20160324_2660.pdf', 'do_20160322_2659.pdf', 'do_20160319_2658.pdf', 'do_20160317_2657.pdf', 'do_20160315_2656.pdf', 'do_20160312_2655.pdf', 'do_20160310_2654.pdf', 'do_20160308_2653.pdf', 'do_20160305_2652.pdf', 'do_20160303_2651.pdf', 'do_20160301_2650.pdf', 'do_20160227_2649.pdf', 'do_20160225_2648.pdf', 'do_20160223_2647.pdf', 'do_20160220_2646.pdf', 'do_20160218_2645.pdf', 'do_20160216_2644.pdf', 'do_20160213_2643.pdf', 'do_20160211_2642.pdf', 'do_20160206_2641.pdf', 'do_20160204_2640.pdf', 'do_20160202_2639.pdf', 'do_20160130_2638.pdf', 'do_20160128_2637.pdf', 'do_20160126_2636.pdf', 'do_20160123_2635.pdf', 'do_20160121_2634.pdf', 'do_20160119_2633.pdf', 'do_20160116_2632.pdf', 'do_20160114_2631.pdf', 'do_20160112_2630.pdf', 'do_20160109_2629.pdf', 'do_20160107_2628.pdf', 'do_20160105_2627.pdf']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "links = []\n",
    "for m in ['12', '11', '10', '09', '08', '07', '06', '05', '04', '03', '02', '01']:\n",
    "    links_ = obter_links_diarios_oficiais('2016', m)\n",
    "    links.extend(links_)\n",
    "\n",
    "arquivos = obter_lista_de_dos(links)\n",
    "print(arquivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-0614316bb36f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mobter_diario_oficial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/arquivos/sist_diariooficial/2022/06/do_20220614_3563.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-e0c9c0809d3e>\u001b[0m in \u001b[0;36mobter_diario_oficial\u001b[0;34m(link, diretorio_destino)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mnome_arquivo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiretorio_destino\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnome_arquivo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mresposta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'https://www2.bauru.sp.gov.br{link}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "obter_diario_oficial('/arquivos/sist_diariooficial/2022/06/do_20220614_3563.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
